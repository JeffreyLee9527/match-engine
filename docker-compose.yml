version: '3.8'

services:
  # MySQL数据库
  mysql:
    image: mysql:8.0
    container_name: spark-mysql
    restart: unless-stopped
    ports:
      - "${MYSQL_PORT:-3306}:3306"
    environment:
      - MYSQL_ROOT_PASSWORD=${MYSQL_ROOT_PASSWORD:-root}
      - MYSQL_DATABASE=${MYSQL_DATABASE:-spark_match_engine}
      - MYSQL_USER=${MYSQL_USER:-spark}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD:-spark}
      - TZ=Asia/Shanghai
    volumes:
      - mysql_data:/var/lib/mysql
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    command:
      - --character-set-server=utf8mb4
      - --collation-server=utf8mb4_unicode_ci
      - --default-authentication-plugin=mysql_native_password
      - --max_connections=1000
      - --innodb_buffer_pool_size=1G
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost", "-u", "root", "-p${MYSQL_ROOT_PASSWORD:-root}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Zookeeper (Kafka依赖)
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: spark-zookeeper
    restart: unless-stopped
    ports:
      - "${ZOOKEEPER_PORT:-2181}:2181"
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
      - ZOOKEEPER_SYNC_LIMIT=2
    volumes:
      - zookeeper_data:/var/lib/zookeeper/data
      - zookeeper_log:/var/lib/zookeeper/log
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "2181"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Kafka
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: spark-kafka
    restart: unless-stopped
    ports:
      - "${KAFKA_PORT:-9092}:9092"
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${KAFKA_ADVERTISED_HOST:-localhost}:${KAFKA_PORT:-9092},PLAINTEXT_INTERNAL://kafka:9093
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      - KAFKA_INTER_BROKER_LISTENER_NAME=PLAINTEXT_INTERNAL
      - KAFKA_LISTENERS=PLAINTEXT://0.0.0.0:9092,PLAINTEXT_INTERNAL://0.0.0.0:9093
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_AUTO_CREATE_TOPICS_ENABLE=true
      - KAFKA_NUM_PARTITIONS=${KAFKA_PARTITIONS:-3}
      - KAFKA_DEFAULT_REPLICATION_FACTOR=1
      - KAFKA_LOG_RETENTION_HOURS=168
      - KAFKA_LOG_SEGMENT_BYTES=1073741824
      - KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS=300000
    volumes:
      - kafka_data:/var/lib/kafka/data
    depends_on:
      zookeeper:
        condition: service_healthy
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9093"]
      interval: 30s
      timeout: 10s
      retries: 5

  # 订单服务
  order-service:
    build:
      context: ./order-service
      dockerfile: Dockerfile
    image: spark-match-engine/order-service:latest
    container_name: spark-order-service
    restart: unless-stopped
    ports:
      - "${ORDER_SERVICE_PORT:-8081}:8081"
    environment:
      - SPRING_PROFILES_ACTIVE=${SPRING_PROFILES_ACTIVE:-docker}
      - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/${MYSQL_DATABASE:-spark_match_engine}?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=Asia/Shanghai&allowPublicKeyRetrieval=true
      - SPRING_DATASOURCE_USERNAME=${MYSQL_USER:-spark}
      - SPRING_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-spark}
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9093
      - JAVA_OPTS=${ORDER_SERVICE_JAVA_OPTS:--Xms512m -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200}
      - TZ=Asia/Shanghai
    volumes:
      - order_service_logs:/logs
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8081/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # 撮合引擎服务
  match-engine-service:
    build:
      context: ./match-engine-service
      dockerfile: Dockerfile
    image: spark-match-engine/match-engine-service:latest
    container_name: spark-match-engine-service
    restart: unless-stopped
    ports:
      - "${MATCH_ENGINE_PORT:-8082}:8082"
    environment:
      - SPRING_PROFILES_ACTIVE=${SPRING_PROFILES_ACTIVE:-docker}
      - SPRING_KAFKA_BOOTSTRAP_SERVERS=kafka:9093
      - SPRING_DATASOURCE_URL=jdbc:mysql://mysql:3306/${MYSQL_DATABASE:-spark_match_engine}?useUnicode=true&characterEncoding=utf8&useSSL=false&serverTimezone=Asia/Shanghai&allowPublicKeyRetrieval=true
      - SPRING_DATASOURCE_USERNAME=${MYSQL_USER:-spark}
      - SPRING_DATASOURCE_PASSWORD=${MYSQL_PASSWORD:-spark}
      - WAL_BASE_PATH=/data/wal
      - WAL_MAX_FILE_SIZE=${WAL_MAX_FILE_SIZE:-104857600}
      - WAL_INSTANCE_ID=${WAL_INSTANCE_ID:-default}
      - SNAPSHOT_BASE_PATH=/data/snapshot
      - SNAPSHOT_INTERVAL=${SNAPSHOT_INTERVAL:-300000}
      - MATCH_ENGINE_ORDERBOOK_UPDATE_DEPTH=${MATCH_ENGINE_ORDERBOOK_UPDATE_DEPTH:-5}
      - JAVA_OPTS=${MATCH_ENGINE_JAVA_OPTS:--Xms1g -Xmx4g -XX:+UseG1GC -XX:MaxGCPauseMillis=200}
      - TZ=Asia/Shanghai
    volumes:
      - wal_data:/data/wal
      - snapshot_data:/data/snapshot
      - match_engine_logs:/logs
    depends_on:
      mysql:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - spark-network
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8082/actuator/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

networks:
  spark-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  mysql_data:
    driver: local
  zookeeper_data:
    driver: local
  zookeeper_log:
    driver: local
  kafka_data:
    driver: local
  wal_data:
    driver: local
  snapshot_data:
    driver: local
  order_service_logs:
    driver: local
  match_engine_logs:
    driver: local
